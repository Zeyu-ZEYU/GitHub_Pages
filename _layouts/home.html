<center>
<img src="assets/zeyu.jpg" width="150px"/>
<h1 id="zeyu-zhang">Zeyu Zhang</h1>
<p><strong>E-mail: <a href="mailto:zeyuzhang@meta.com">zeyuzhang@meta.com</a> / <a href="mailto:qxc4fh@virginia.edu">qxc4fh@virginia.edu</a> / <a href="mailto:zhang@zeyu.tw">zhang@zeyu.tw</a></strong></p>
</center>

<hr>

<h2 id="Introduction">INTRODUCTION</h2>
I am a PhD student at the University of Virginia (UVA), focusing on systems for training, inference, and evaluation of Large Language Models (LLMs) and recommendation models. My research primarily centers on optimizing long-context models and improving the communication, computation, and memory efficiency of KV cache. I also work on mitigating straggler issues in large-scale Machine Learning (ML) training. Prior to my PhD, I worked on network communication optimization, including user-space networking stacks and Network Function Virtualization (NFV). Earlier in my career, I also conducted research in recommender systems and algorithms.

<h2 id="experiences">EXPERIENCES</h2>
<ul>
    <li><strong>Meta (Formerly Facebook), Sunnyvale, California, USA</strong> (06/2025-Now)
        <ul>
            <li><b>Research Scientist Intern (AI Systems Machine Learning)</b></li>
            <li>Working on systems for large language and recommendation models.</li>
        </ul>
    </li>

    <li><strong>Harvard University, Boston, Massachusetts, USA</strong> (03/2024-08/2024)
        <ul>
            <li><b>Visiting Researcher</b></li>
            <li>Worked on systems for Large Language Models (LLMs). The main topic was LLM KV cache quantization, and we propsed homomorphic quantization for LLM KV cache to deal with communication, computation, and memory issues in disaggregated LLM serving.</li>
        </ul>
    </li>

    <li><strong>Microsoft, Seattle, Washington, USA</strong> (05/2023-08/2023, 09/2024-12/2024)
        <ul>
            <li><b>Visiting Researcher</b></li>
            <li>Worked with DeepSpeed on LLM training, especially on long-context-model training.</li>
            <li>Worked with Azure on multi-modality model serving.</li>
        </ul>
    </li>

    <li><strong>University of Virginia, Charlottesville, Virginia, USA</strong> (08/2021-Now)
        <ul>
            <li><b>PhD in Computer Science</b></li>
            <li>Working on systems for AI.</li>
            <li>Worked on LLM KV cache optimization and proposed ZACK to reduce KV cache size in the hidden size dimension, which is orthogonal to quantization and token eviction based methods. I also enhanced the self-attention kernel used for ZACK.</li>
            <li>Worked on long-context-model inference and proposed CSPS and PecSched for efficient long-context-model serving.</li>
            <li>Also worked on straggler problems in Machine Learning (ML) training.</li>
        </ul>
    </li>

    <li><strong>Intel, Shanghai, China</strong> (06/2019-11/2019)
        <ul>
            <li><b>Intern in Network and Custom Logic Group (NCLG)</b></li>
            <li>Worked on user-space networking stack (collaborating with Cisco). I optimized NGINX based on open-source high-performance packet processing framework VPP to increase its throughput, achieve good scalability, reduce latency and reduce CPU usage.</li>
        </ul>
    </li>

    <li><strong>Shanghai Jiao Tong University, Shanghai, China</strong> (09/2017-03/2020)
        <ul>
            <li><b>Master in Software Engineering</b></li>
            <li>Worked on network function virtualization (NFV).</li>
        </ul>
    </li>

    <li><strong>Wuhan University, Wuhan, China</strong> (09/2013-06/2017)
        <ul>
            <li><b>Bachelor in Software Engineering</b></li>
            <li>Made identification of consumer groups in shopping malls and promoted the maximization of merchants' profits by predicting consumer groups' group behaviors.</li>
            <li>I designed an improved Apriori Algorithm that is able to efficiently conduct trajectory prediction in large shopping malls.</li>
        </ul>
    </li>
</ul>

<h2 id="research_papers">RESEARCH PAPERS</h2>
<ul>
    <li>Zeyu Zhang and Haiying Shen. PecSched: Preemptive and efficient cluster scheduling for LLM inference. arXiv: 2409.15104v2, 2025. [<a href="https://arxiv.org/abs/2409.15104v2">Link</a>]</li>
    <li>Zeyu Zhang, Haiying Shen, Shay Vargaftik, Ran Ben Basat, Michael Mitzenmacher, and Minlan Yu. HACK: Homomorphic acceleration via compression of the key-value cache for disaggregated LLM inference. SIGCOMM Shorts, 2025. [<a href="assets/hack_short_sigcomm_25.pdf">Link</a>]</li>
    <li>Zeyu Zhang and Haiying Shen. FDC: Fast KV dimensionality compression for efficient LLM inference. arXiv: 2408.04107v3, 2025. [<a href="https://arxiv.org/abs/2408.04107v3">Link</a>]</li>
    <li>Suraiya Tairin, Zeyu Zhang, and Haiying Shen. Revisiting the straggling problem in GPU-based distributed deep learning training. In 2025 34th International Conference on Computer Communications and Networks (ICCCN), pages 1-9, 2025.</li>
    <li>Haoran Qiu, Anish Biswas, Zihan Zhao, Jayashree Mohan, Alind Khare, Esha Choukse, Íñigo Goiri, Zeyu Zhang, Haiying Shen, Chetan Bansal, Ramachandran Ramjee, and Rodrigo Fonseca. ModServe: Scalable and resource-efficient large multimodal model serving. arXiv: 2502.00937v2, 2025. [<a href="https://arxiv.org/abs/2502.00937v2">Link</a>]</li>
    <li>Haiying Shen and Zeyu Zhang. Deep learning training job scheduling for proactive straggler reduction. In 2025 IEEE 25th International Symposium on Cluster, Cloud and Internet Computing (CCGrid), pages 1-12, 2025.</li>
    <li>Zeyu Zhang, Haiying Shen, Shay Vargaftik, Ran Ben Basat, Michael Mitzenmacher, and Minlan Yu. HACK: Homomorphic acceleration via compression of the key-value cache for disaggregated LLM inference. arXiv: 2502.03589v1, 2025. [<a href="https://arxiv.org/abs/2502.03589v1">Link</a>]</li>
    <li>Zeyu Zhang and Haiying Shen. ZACK: Zero-overhead LLM inference acceleration via dimensionality compression of the key-value cache. arXiv: 2408.04107v2, 2024. [<a href="https://arxiv.org/abs/2408.04107v2">Link</a>]</li>
    <li>Zeyu Zhang and Haiying Shen. CSPS: A communication-efficient sequence-parallelism based serving system for transformer based models with long prompts. arXiv: 2409.15104v1, 2024. [<a href="https://arxiv.org/abs/2409.15104v1">Link</a>]</li>
    <li>Suraiya Tairin, Haiying Shen, and Zeyu Zhang. Embracing uncertainty for equity in resource allocation in ML training. In Proceedings of the 52nd International Conference on Parallel Processing, ICPP '23, page 423-432, New York, NY, USA, 2023. Association for Computing Machinery.</li>
    <li>Zeyu Zhang and Weiping Zhu. Location and motion prediction of consumers in a large shopping mall. In 2017 Fifth International Conference on Advanced Cloud and Big Data (CBD), pages 250-255, 2017.</li>
</ul>

<h2 id="patents">PATENTS</h2>
<ul>
    <li>A Network Request Processing System and Method (Jian Li, Zeyu Zhang, Haibing Guan) (Chinese Patent Number: 202010059255.0)</li>
</ul>
